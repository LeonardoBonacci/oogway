{
    "name": "jdbc_source_enrichment_03",
    "config": {
            "_comment": "The JDBC connector class. Don't change this if you want to use the JDBC Source.",
            "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",

            "_comment": "How to serialise the value of keys - here use the Confluent Avro serialiser. Note that the JDBC Source Connector always returns null for the key ",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",

            "_comment": "As above, but for the value of the message.",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
			"value.converter.schemas.enable": "false",

            "_comment": " --- JDBC-specific configuration below here  --- ",
            "_comment": "JDBC connection URL. This will vary by RDBMS. Consult your manufacturer's handbook for more information",
			"connection.url": "jdbc:postgresql://localhost:5432/demo?user=demo&password=pw",

            "_comment": "Which table(s) to include",
            "table.whitelist": "enquiryevent",

            "_comment": "Pull all rows based on an timestamp column. You can also do bulk or incrementing column-based extracts. For more information, see http://docs.confluent.io/current/connect/connect-jdbc/docs/source_config_options.html#mode",
            "mode": "timestamp",

            "_comment": "Which column has the timestamp value to use?  ",
            "timestamp.column.name": "update_ts",

            "_comment": "If the column is not defined as NOT NULL, tell the connector to ignore this  ",
            "validate.non.null": "false",

            "_comment": "The Kafka topic will be made up of this prefix, plus the table name  ",
            "topic.prefix": "postum-",

            "_comment": "---- Single Message Transforms ----",
            "transforms":"createKey,extractString,dropFieldD1,dropFieldD2",
            "transforms.createKey.type":"org.apache.kafka.connect.transforms.ValueToKey",
            "transforms.createKey.fields":"apikey",
            "transforms.extractString.type":"org.apache.kafka.connect.transforms.ExtractField$Key",
            "transforms.extractString.field":"apikey",
            "transforms.dropFieldD1.type":"org.apache.kafka.connect.transforms.ReplaceField$Value",  
			"transforms.dropFieldD1.blacklist":"create_ts",
            "transforms.dropFieldD2.type":"org.apache.kafka.connect.transforms.ReplaceField$Value",  
			"transforms.dropFieldD2.blacklist":"update_ts"
    }
}
